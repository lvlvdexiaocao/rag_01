import streamlit as st

from rag_chain import get_response_from_rag


# é¡µé¢æ ‡é¢˜
st.title("RAG ChatBot")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []

# # æ˜¾ç¤ºå†å²æ¶ˆæ¯
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])

MODEL_NAMES_OLLAMA = ["qwen2.5:3b"]
MODEL_NAMES_GLM = ["glm-4.6", "glm-4.5-flash"]
MODEL_NAMES_DEEPSEEK = ["deepseek-chat"]

with st.sidebar:
    st.header("YIYIYIYIY")
    with st.popover("Settings",use_container_width=True):
        # allow_web_search = st.checkbox("Allow Web Search")
        model = st.selectbox("LLM to use", options=["Ollama", "GLM", "DEEPSEEK"])
        if model == "Ollama":
            selected_model = st.selectbox("Select Groq Model:", MODEL_NAMES_OLLAMA)
        elif model == "GLM":
            selected_model = st.selectbox("Select OpenAI Model:", MODEL_NAMES_GLM)
        elif model == "DEEPSEEK":
            selected_model = st.selectbox("Select OpenAI Model:", MODEL_NAMES_DEEPSEEK)

# æ˜¾ç¤ºå†å²æ¶ˆæ¯ï¼ˆåŒ…å«å¯å±•å¼€çš„æ£€ç´¢ä¾æ®ï¼‰
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        # å¦‚æœæ˜¯åŠ©æ‰‹å›å¤ä¸”æœ‰ sourcesï¼Œæ˜¾ç¤ºå¯å±•å¼€çš„æ£€ç´¢å†…å®¹
        if msg["role"] == "assistant" and "sources" in msg:
            with st.expander("ğŸ” æŸ¥çœ‹æ£€ç´¢åˆ°çš„æ¡æ¬¾ä¾æ®"):
                sources = msg["sources"]
                if sources:
                    for i, doc in enumerate(sources):
                        source_name = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
                        with st.expander(f"ğŸ“„ ç‰‡æ®µ {i+1} | æ¥æºï¼š{source_name}"):
                            st.text(doc.page_content)
                else:
                    st.write("âŒ æœªæ£€ç´¢åˆ°ç›¸å…³æ¡æ¬¾ã€‚")


# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„ä¿é™©é—®é¢˜ï¼Œä¾‹å¦‚ï¼šâ€œç­‰å¾…æœŸå†…ç¡®è¯Šä¹³è…ºåŸä½ç™Œèƒ½èµ”å—ï¼Ÿâ€"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)


    # è°ƒç”¨ RAGï¼Œè·å–å®Œæ•´ç»“æœ
    answer_text, ref_docs = get_response_from_rag(query=prompt, provider=model, llm_id=selected_model)


    # ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯ï¼ˆå« sourcesï¼‰
    st.session_state.messages.append({
        "role": "assistant",
        "content": answer_text,
        "sources": ref_docs
    })
    # ä¿å­˜å¹¶æ˜¾ç¤ºå›ç­”
    # æ˜¾ç¤ºå½“å‰å›ç­”
    with st.chat_message("assistant"):
        st.markdown(answer_text)

    # æ˜¾ç¤ºå½“å‰å›ç­”çš„æ£€ç´¢ä¾æ®ï¼ˆæŠ˜å ï¼‰
    with st.expander("ğŸ” æŸ¥çœ‹æ£€ç´¢åˆ°çš„æ¡æ¬¾ä¾æ®"):
        if ref_docs:
            for i, doc in enumerate(ref_docs):
                source_name = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
                with st.expander(f"ğŸ“„ ç‰‡æ®µ {i+1} | æ¥æºï¼š{source_name}"):
                    st.text(doc.page_content)
        else:
            st.write("âŒ æœªæ£€ç´¢åˆ°ç›¸å…³æ¡æ¬¾ã€‚")
